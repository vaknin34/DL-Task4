{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 4\n",
    "All previous instructions hold. In addition, if you are using GPU, you must check that your code also runs on a CPU. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import copy\n",
    "import zipfile\n",
    "import time\n",
    "import random\n",
    "import collections\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import pickle\n",
    "import requests\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms \n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as T\n",
    "import torchvision.models as models\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "\n",
    "print(torch.__version__)\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Neural Networks (40 points)\n",
    "\n",
    "Understanding and implementing the RNN cell. As you learned in class, the RNN has a certain structure that allows it to accept the previous hidden state the current input, and output an hidden state and an output vector. The RNN cell uses the same weights for all time steps, much like convolution uses the same weights for all the batches in the image. Even though you already are familiar with PyTorch, implementing the RNN you make sure you understand how this pivotal architecture works."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rel_error(x, y):\n",
    "    \"\"\" returns relative error \"\"\"\n",
    "    return np.max(np.abs(x - y) / (np.maximum(1e-8, np.abs(x) + np.abs(y))))\n",
    "\n",
    "def eval_numerical_gradient_array(f, x, df, h=1e-5):\n",
    "    \"\"\"\n",
    "    Evaluate a numeric gradient for a function that accepts a numpy\n",
    "    array and returns a numpy array.\n",
    "    \"\"\"\n",
    "    grad = np.zeros_like(x)\n",
    "    it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])\n",
    "    while not it.finished:\n",
    "        ix = it.multi_index\n",
    "\n",
    "        oldval = x[ix]\n",
    "        x[ix] = oldval + h\n",
    "        pos = f(x).copy()\n",
    "        x[ix] = oldval - h\n",
    "        neg = f(x).copy()\n",
    "        x[ix] = oldval\n",
    "\n",
    "        grad[ix] = np.sum((pos - neg) * df) / (2 * h)\n",
    "        it.iternext()\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  RNN: step forward (10 points)\n",
    "\n",
    "First implement the function `rnn_step_forward` which implements the forward pass for a single timestep of a recurrent neural network. After doing so run the following to check your implementation. You should see errors less than 1e-7."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rnn_step_forward(x, prev_h, Wx, Wh, b):\n",
    "    \"\"\"\n",
    "    Run the forward pass for a single timestep of an RNN that uses a tanh\n",
    "    activation function.\n",
    "\n",
    "    The input data has dimension D, the hidden state has dimension H, and we use\n",
    "    a minibatch size of N.\n",
    "\n",
    "    Inputs:\n",
    "    - x: Input data for this timestep, of shape (N, D).\n",
    "    - prev_h: Hidden state from previous timestep, of shape (N, H)\n",
    "    - Wx: Weight matrix for input-to-hidden connections, of shape (D, H)\n",
    "    - Wh: Weight matrix for hidden-to-hidden connections, of shape (H, H)\n",
    "    - b: Biases of shape (H,)\n",
    "\n",
    "    Returns a tuple of:\n",
    "    - next_h: Next hidden state, of shape (N, H)\n",
    "    - cache: Tuple of values needed for the backward pass.\n",
    "    \"\"\"\n",
    "    next_h, cache = None, None\n",
    "    ##############################################################################\n",
    "    # TODO: Implement a single forward step for the RNN. Store the next  #\n",
    "    # hidden state and any values you need for the backward pass in the next_h   #\n",
    "    # and cache variables respectively.                                          #\n",
    "    ##############################################################################\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    ##############################################################################\n",
    "    #                               END OF YOUR CODE                             #\n",
    "    ##############################################################################\n",
    "    return next_h, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N, D, H = 3, 10, 4\n",
    "\n",
    "x = np.linspace(-0.4, 0.7, num=N*D).reshape(N, D)\n",
    "prev_h = np.linspace(-0.2, 0.5, num=N*H).reshape(N, H)\n",
    "Wx = np.linspace(-0.1, 0.9, num=D*H).reshape(D, H)\n",
    "Wh = np.linspace(-0.3, 0.7, num=H*H).reshape(H, H)\n",
    "b = np.linspace(-0.2, 0.4, num=H)\n",
    "\n",
    "next_h, _ = rnn_step_forward(x, prev_h, Wx, Wh, b)\n",
    "expected_next_h = np.asarray([\n",
    "  [-0.58172089, -0.50182032, -0.41232771, -0.31410098],\n",
    "  [ 0.66854692,  0.79562378,  0.87755553,  0.92795967],\n",
    "  [ 0.97934501,  0.99144213,  0.99646691,  0.99854353]])\n",
    "\n",
    "print('next_h error: ', rel_error(expected_next_h, next_h))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#RNN: step backward (10 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rnn_step_backward(dnext_h, cache):\n",
    "    \"\"\"\n",
    "    Backward pass for a single timestep of an RNN.\n",
    "\n",
    "    Inputs:\n",
    "    - dnext_h: Gradient of loss with respect to next hidden state\n",
    "    - cache: Cache object from the forward pass\n",
    "\n",
    "    Returns a tuple of:\n",
    "    - dx: Gradients of input data, of shape (N, D)\n",
    "    - dprev_h: Gradients of previous hidden state, of shape (N, H)\n",
    "    - dWx: Gradients of input-to-hidden weights, of shape (D, H)\n",
    "    - dWh: Gradients of hidden-to-hidden weights, of shape (H, H)\n",
    "    - db: Gradients of bias vector, of shape (H,)\n",
    "    \"\"\"\n",
    "    dx, dprev_h, dWx, dWh, db = None, None, None, None, None\n",
    "    ##############################################################################\n",
    "    # TODO: Implement the backward pass for a single step of a RNN.      #\n",
    "    #                                                                            #\n",
    "    # HINT: For the tanh function, you can compute the local derivative in terms #\n",
    "    # of the output value from tanh.                                             #\n",
    "    ##############################################################################\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    ##############################################################################\n",
    "    #                               END OF YOUR CODE                             #\n",
    "    ##############################################################################\n",
    "    return dx, dprev_h, dWx, dWh, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1337)\n",
    "N, D, H = 4, 5, 6\n",
    "x = np.random.randn(N, D)\n",
    "h = np.random.randn(N, H)\n",
    "Wx = np.random.randn(D, H)\n",
    "Wh = np.random.randn(H, H)\n",
    "b = np.random.randn(H)\n",
    "\n",
    "out, cache = rnn_step_forward(x, h, Wx, Wh, b)\n",
    "\n",
    "dnext_h = np.random.randn(*out.shape)\n",
    "\n",
    "fx = lambda x: rnn_step_forward(x, h, Wx, Wh, b)[0]\n",
    "fh = lambda prev_h: rnn_step_forward(x, h, Wx, Wh, b)[0]\n",
    "fWx = lambda Wx: rnn_step_forward(x, h, Wx, Wh, b)[0]\n",
    "fWh = lambda Wh: rnn_step_forward(x, h, Wx, Wh, b)[0]\n",
    "fb = lambda b: rnn_step_forward(x, h, Wx, Wh, b)[0]\n",
    "\n",
    "dx_num = eval_numerical_gradient_array(fx, x, dnext_h)\n",
    "dprev_h_num = eval_numerical_gradient_array(fh, h, dnext_h)\n",
    "dWx_num = eval_numerical_gradient_array(fWx, Wx, dnext_h)\n",
    "dWh_num = eval_numerical_gradient_array(fWh, Wh, dnext_h)\n",
    "db_num = eval_numerical_gradient_array(fb, b, dnext_h)\n",
    "\n",
    "dx, dprev_h, dWx, dWh, db = rnn_step_backward(dnext_h, cache)\n",
    "\n",
    "print('dx error: ', rel_error(dx_num, dx))\n",
    "print('dprev_h error: ', rel_error(dprev_h_num, dprev_h))\n",
    "print('dWx error: ', rel_error(dWx_num, dWx))\n",
    "print('dWh error: ', rel_error(dWh_num, dWh))\n",
    "print('db error: ', rel_error(db_num, db))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN: forward (10 points)\n",
    "Now that you have implemented the forward and backward passes for a single timestep of an RNN, you will combine these pieces to implement an RNN that process an entire sequence of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rnn_forward(x, h0, Wx, Wh, b):\n",
    "    \"\"\"\n",
    "    Run an RNN forward on an entire sequence of data. We assume an input\n",
    "    sequence composed of T vectors, each of dimension D. The RNN uses a hidden\n",
    "    size of H, and we work over a minibatch containing N sequences. After running\n",
    "    the RNN forward, we return the hidden states for all timesteps.\n",
    "\n",
    "    Inputs:\n",
    "    - x: Input data for the entire timeseries, of shape (N, T, D).\n",
    "    - h0: Initial hidden state, of shape (N, H)\n",
    "    - Wx: Weight matrix for input-to-hidden connections, of shape (D, H)\n",
    "    - Wh: Weight matrix for hidden-to-hidden connections, of shape (H, H)\n",
    "    - b: Biases of shape (H,)\n",
    "\n",
    "    Returns a tuple of:\n",
    "    - h: Hidden states for the entire timeseries, of shape (N, T, H).\n",
    "    - cache: Values needed in the backward pass\n",
    "    \"\"\"\n",
    "    h, cache = None, None\n",
    "    ##############################################################################\n",
    "    # TODO: Implement forward pass for a RNN running on a sequence of    #\n",
    "    # input data. You should use the rnn_step_forward function that you defined  #\n",
    "    # above. You can use a for loop to help compute the forward pass.            #\n",
    "    ##############################################################################\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    ##############################################################################\n",
    "    #                               END OF YOUR CODE                             #\n",
    "    ##############################################################################\n",
    "    return h, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N, T, D, H = 2, 3, 4, 5\n",
    "\n",
    "x = np.linspace(-0.1, 0.3, num=N*T*D).reshape(N, T, D)\n",
    "h0 = np.linspace(-0.3, 0.1, num=N*H).reshape(N, H)\n",
    "Wx = np.linspace(-0.2, 0.4, num=D*H).reshape(D, H)\n",
    "Wh = np.linspace(-0.4, 0.1, num=H*H).reshape(H, H)\n",
    "b = np.linspace(-0.7, 0.1, num=H)\n",
    "\n",
    "h, _ = rnn_forward(x, h0, Wx, Wh, b)\n",
    "expected_h = np.asarray([\n",
    "  [\n",
    "    [-0.42070749, -0.27279261, -0.11074945,  0.05740409,  0.22236251],\n",
    "    [-0.39525808, -0.22554661, -0.0409454,   0.14649412,  0.32397316],\n",
    "    [-0.42305111, -0.24223728, -0.04287027,  0.15997045,  0.35014525],\n",
    "  ],\n",
    "  [\n",
    "    [-0.55857474, -0.39065825, -0.19198182,  0.02378408,  0.23735671],\n",
    "    [-0.27150199, -0.07088804,  0.13562939,  0.33099728,  0.50158768],\n",
    "    [-0.51014825, -0.30524429, -0.06755202,  0.17806392,  0.40333043]]])\n",
    "print('h error: ', rel_error(expected_h, h))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN: backward (10 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rnn_backward(dh, cache):\n",
    "    \"\"\"\n",
    "    Compute the backward pass for an RNN over an entire sequence of data.\n",
    "\n",
    "    Inputs:\n",
    "    - dh: Upstream gradients of all hidden states, of shape (N, T, H)\n",
    "\n",
    "    Returns a tuple of:\n",
    "    - dx: Gradient of inputs, of shape (N, T, D)\n",
    "    - dh0: Gradient of initial hidden state, of shape (N, H)\n",
    "    - dWx: Gradient of input-to-hidden weights, of shape (D, H)\n",
    "    - dWh: Gradient of hidden-to-hidden weights, of shape (H, H)\n",
    "    - db: Gradient of biases, of shape (H,)\n",
    "    \"\"\"\n",
    "    dx, dh0, dWx, dWh, db = None, None, None, None, None\n",
    "    ##############################################################################\n",
    "    # TODO: Implement the backward pass for a RNN running an entire      #\n",
    "    # sequence of data. You should use the rnn_step_backward function that you   #\n",
    "    # defined above. You can use a for loop to help compute the backward pass.   #\n",
    "    ##############################################################################\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    ##############################################################################\n",
    "    #                               END OF YOUR CODE                             #\n",
    "    ##############################################################################\n",
    "    return dx, dh0, dWx, dWh, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1337)\n",
    "\n",
    "N, D, T, H = 2, 3, 10, 5\n",
    "\n",
    "x = np.random.randn(N, T, D)\n",
    "h0 = np.random.randn(N, H)\n",
    "Wx = np.random.randn(D, H)\n",
    "Wh = np.random.randn(H, H)\n",
    "b = np.random.randn(H)\n",
    "\n",
    "out, cache = rnn_forward(x, h0, Wx, Wh, b)\n",
    "\n",
    "dout = np.random.randn(*out.shape)\n",
    "\n",
    "dx, dh0, dWx, dWh, db = rnn_backward(dout, cache)\n",
    "\n",
    "fx = lambda x: rnn_forward(x, h0, Wx, Wh, b)[0]\n",
    "fh0 = lambda h0: rnn_forward(x, h0, Wx, Wh, b)[0]\n",
    "fWx = lambda Wx: rnn_forward(x, h0, Wx, Wh, b)[0]\n",
    "fWh = lambda Wh: rnn_forward(x, h0, Wx, Wh, b)[0]\n",
    "fb = lambda b: rnn_forward(x, h0, Wx, Wh, b)[0]\n",
    "\n",
    "dx_num = eval_numerical_gradient_array(fx, x, dout)\n",
    "dh0_num = eval_numerical_gradient_array(fh0, h0, dout)\n",
    "dWx_num = eval_numerical_gradient_array(fWx, Wx, dout)\n",
    "dWh_num = eval_numerical_gradient_array(fWh, Wh, dout)\n",
    "db_num = eval_numerical_gradient_array(fb, b, dout)\n",
    "\n",
    "print('dx error: ', rel_error(dx_num, dx))\n",
    "print('dh0 error: ', rel_error(dh0_num, dh0))\n",
    "print('dWx error: ', rel_error(dWx_num, dWx))\n",
    "print('dWh error: ', rel_error(dWh_num, dWh))\n",
    "print('db error: ', rel_error(db_num, db))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-04T12:18:23.856501Z",
     "start_time": "2022-11-04T12:18:23.764777Z"
    }
   },
   "source": [
    "## Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-04T12:18:23.856501Z",
     "start_time": "2022-11-04T12:18:23.764777Z"
    }
   },
   "source": [
    "##**Question:** \n",
    "When using an RNN on long sequences, what could happen? What causes this to happen?   **(5 Points)**\n",
    "\n",
    "**Your answer:** *Fill this in*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-04T12:18:23.856501Z",
     "start_time": "2022-11-04T12:18:23.764777Z"
    }
   },
   "source": [
    "##**Question:** \n",
    "Could this problem be solved by a different model? How does it accomplish this?   **(5 Points)**\n",
    "\n",
    "**Your answer:** *Fill this in*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Image Captioning in PyTorch (50 points)\n",
    "\n",
    "The goal of image captioning is to describe a given image using natural language. Using neural networks, we can partition the problem into two separate challenges. First, we need to extract meaningful features regarding the image that would help us describe it. Second, we need to generate a sequence of words that best fit those features. Luckily, the flexability of neural networks allows us to take a CNN architecture and connect it directly to a LSTM network. We only need to provide proper labels to train the new network we created. For this exercise, you will be provided with pretrained networks for both feature extraction and sentence generation, and you will connect the different components needed to make image captioning work.\n",
    "\n",
    "First, we define the feature extractor and the recurrent model seperately. The feature extractor takes an image and produces a vector representation of the image features. As those features hold information about the image, we will use that vector as the input for our recurrent model. The RNN will produce the image captioning using an LSTM architecture. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# unzipping the pretrained models\n",
    "with zipfile.ZipFile(os.path.join('models', 'pretrained_model.zip'), 'r') as zip_ref:\n",
    "    zip_ref.extractall('models')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unzipping vocabulary\n",
    "with zipfile.ZipFile(os.path.join('data', 'vocab.zip'), 'r') as zip_ref:\n",
    "    zip_ref.extractall('data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_path = 'models/encoder-5-3000.pkl'\n",
    "lstm_path = 'models/decoder-5-3000.pkl'\n",
    "vocab_path   = 'data/vocab.pkl'\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Use the following code to check if all the files are in place\n",
    "print(\"conv_path \", '✓' if os.path.isfile(conv_path) == True else '✗') \n",
    "print(\"lstm_path \", '✓' if os.path.isfile(lstm_path) == True else '✗') \n",
    "print(\"vocab_path \", '✓' if os.path.isfile(vocab_path) == True else '✗') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-04T12:24:56.821196Z",
     "start_time": "2022-11-04T12:24:56.758302Z"
    }
   },
   "source": [
    "## Implementing image captioning model **(40 points)**.\n",
    "\n",
    "As training a multimodal classifier could take some time and resources, we spared you the training phase this time. In this exercise, we use a pretrained model to solve the image captioning task. Using pretrained models is a common practice in the deep learning community and it's important to be aware of such techniques to save time and energy. In previous cells, we unzipped the necessary files, but in order to be able to load the models (and then use them) it is required to build the same PyTorch model as the pretrained model.\n",
    "\n",
    "**ConvNet architecture:** resnet152 (without last fc layer) -> fc layer -> BatchNorm1d\n",
    "\n",
    "**LSTM architecture:** LSTM -> linear -> embed\n",
    "\n",
    "We added more detailed instructions in the next cells, please make sure you follow them carefully. \n",
    "\n",
    "**Please make sure you construct your models based on the sizes we provided.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_size   = 256      # dimension of word embedding vectors\n",
    "hidden_size  = 512      # dimension of lstm hidden states\n",
    "num_layers   = 1        # number of layers in lstm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet(nn.Module):\n",
    "    def __init__(self, embed_size):\n",
    "        super(ConvNet, self).__init__()\n",
    "        resnet = models.resnet152() # construct an nn.Sequential model without the last resnet152 layer\n",
    "        modules = list(resnet.children())[:-1]\n",
    "        self.resnet = nn.Sequential(*modules)\n",
    "        # make sure you define each of the None parameters\n",
    "        self.linear = None\n",
    "        self.bn = None\n",
    "        #############################################################################\n",
    "        # TO DO:                                                                    #\n",
    "        # create a new sequential model which includes the resnet and               #\n",
    "        # add a new fully connected layer that outputs a vector with the size of    #\n",
    "        # the wanted embedding. Next, you should add a batchnorm layer with         #\n",
    "        # momentum=0.01 (BatchNorm1d parameter).                                    #\n",
    "        # This function has no return value.                                        #\n",
    "        #############################################################################\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        #############################################################################\n",
    "        #                             END OF YOUR CODE                              #\n",
    "        #############################################################################\n",
    "    \n",
    "    def forward(self, images):\n",
    "        #############################################################################\n",
    "        # TO DO:                                                                    #\n",
    "        # Define the forward propagation. You need to pass an image through the     #\n",
    "        # network and extract the feature vector. In this case, when using a        #\n",
    "        # perdefined network, you don't want to change it's weights.                #\n",
    "        # The rest of the layers you defined should accepts gradients for them to   #\n",
    "        # improve during training. Make sure you are inputing a correct shape       #\n",
    "        # to the batchnorm layer.                                                   #\n",
    "        # This function returns the features of the image.                          #\n",
    "        #############################################################################\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        #############################################################################\n",
    "        #                             END OF YOUR CODE                              #\n",
    "        #############################################################################\n",
    "\n",
    "\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self, embed_size, hidden_size, vocab_size, num_layers, max_seq_length=20):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.max_seg_length = max_seq_length\n",
    "        self.embed = None\n",
    "        self.lstm = None\n",
    "        self.linear = None\n",
    "        #############################################################################\n",
    "        # TO DO:                                                                    #\n",
    "        # Define the hyper-parameters and the layers of the pretrained LSTM.        #\n",
    "        # Create an Embedding layer that accepts the output of the                  #\n",
    "        # feature extractor.  Next, the built-in LSTM architecture in PyTorch.nn    #\n",
    "        # with the proper inputs (use the built-in documentation tool in Jupyter    #\n",
    "        # or just look at the official documentation online).                       #\n",
    "        # Define an additional linear layer that comes after the LSTM and outputs   #\n",
    "        # a vector that will support the size of our vocabulary.                    #\n",
    "        # This function has no return value.                                        #\n",
    "        #############################################################################\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        #############################################################################\n",
    "        #                             END OF YOUR CODE                              #\n",
    "        #############################################################################           \n",
    "    \n",
    "    def sample(self, features, states=None):\n",
    "        sampled_ids = []\n",
    "        inputs = features.unsqueeze(1)\n",
    "        #############################################################################\n",
    "        # TO DO:                                                                    #\n",
    "        # Generate captions for a given image features.                             #\n",
    "        # First, obtain the output of the LSTM network.        #\n",
    "        # Next, use the hidden states to obtain the most probable word and store    #\n",
    "        # all the word predictions in the sampled_ids list. Don't forget to update  #\n",
    "        # the inputs for each timestep to continue making predictions based on the  #\n",
    "        # words you are alreaedy predicted.                                         #\n",
    "        # Make sure you keep track of the dimensions of the inputs and outputs,     #\n",
    "        # since PyTorch expects tensors with a batch dimension. You can use the     #\n",
    "        # methods .squeeze() and .unsqueeze()                                       #\n",
    "        # This function returns the list of predicted words.                        #\n",
    "        #############################################################################\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        #############################################################################\n",
    "        #                             END OF YOUR CODE                              #\n",
    "        #############################################################################        \n",
    "\n",
    "        return sampled_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(image_path, transform=None):\n",
    "    image = Image.open(image_path)\n",
    "    image = image.resize([224, 224], Image.Resampling.LANCZOS)\n",
    "    \n",
    "    if transform is not None:\n",
    "        image = transform(image).unsqueeze(0)\n",
    "    \n",
    "    return image\n",
    "\n",
    "class Vocabulary(object):\n",
    "    \"\"\"Simple vocabulary wrapper.\"\"\"\n",
    "    def __init__(self):\n",
    "        self.word2idx = {}\n",
    "        self.idx2word = {}\n",
    "        self.idx = 0\n",
    "\n",
    "    def add_word(self, word):\n",
    "        if not word in self.word2idx:\n",
    "            self.word2idx[word] = self.idx\n",
    "            self.idx2word[self.idx] = word\n",
    "            self.idx += 1\n",
    "\n",
    "    def __call__(self, word):\n",
    "        if not word in self.word2idx:\n",
    "            return self.word2idx['<unk>']\n",
    "        return self.word2idx[word]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.word2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(), \n",
    "    transforms.Normalize((0.485, 0.456, 0.406), \n",
    "                         (0.229, 0.224, 0.225))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(vocab_path, 'rb') as f:\n",
    "    vocab = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build models\n",
    "conv = ConvNet(embed_size).eval()  # eval mode (batchnorm uses moving mean/variance)\n",
    "lstm = LSTM(embed_size, hidden_size, len(vocab), num_layers)\n",
    "conv = conv.to(device)\n",
    "lstm = lstm.to(device)\n",
    "\n",
    "# Load the trained model parameters\n",
    "conv.load_state_dict(torch.load(conv_path))\n",
    "lstm.load_state_dict(torch.load(lstm_path))\n",
    "\n",
    "# Prepare an image\n",
    "image_path = os.path.join('data', 'pic.jpg')\n",
    "image = load_image(image_path, transform)\n",
    "image_tensor = image.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate an caption from the image\n",
    "# Important Note - this piece of code may not work for all implementations. You may need to adjust it a bit so it \n",
    "# will produce the desired output (image + caption)\n",
    "feature = conv(image_tensor)\n",
    "sampled_ids = lstm.sample(feature)\n",
    "sampled_ids = sampled_ids[0].cpu().numpy()\n",
    "\n",
    "# Convert word_ids to words\n",
    "sampled_caption = []\n",
    "for word_id in sampled_ids:\n",
    "    word = vocab.idx2word[word_id]\n",
    "    sampled_caption.append(word)\n",
    "    if word == '<end>':\n",
    "        break\n",
    "sentence = ' '.join(sampled_caption)\n",
    "\n",
    "# Print out the image and the generated caption\n",
    "print(sentence)\n",
    "image = Image.open(image_path)\n",
    "plt.imshow(np.asarray(image));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-04T12:21:49.196557Z",
     "start_time": "2022-11-04T12:21:49.127510Z"
    }
   },
   "source": [
    "## Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-11-04T12:21:49.196557Z",
     "start_time": "2022-11-04T12:21:49.127510Z"
    }
   },
   "source": [
    "##**Question:** \n",
    "Could you think of scenarios where the model fails? Why would that happen? **(10 Points)**\n",
    "\n",
    "We encourage you to test it on different images (jpg). \n",
    "\n",
    "**Your answer:** *Fill this in*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
